//the integration code for the live2d and face api models
//Debashish Buragohain
"use strict";
const webcam = document.getElementById("webcam");
/*
//we are laterally inverting the camera                
webcam.style.cssText = "-moz-transform: scale(-1, 1); \
-webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); \
transform: scale(-1, 1); filter: FlipH;";
*/

//declaring some global variables for use in both the TypeScript and Javascript modules
var rmsValue = 0;
var currentExpression = "";
var xCanvasValue = 0;
var yCanvasValue = 0;
var xCanvasMin = 0;
var yCanvasMin = 0;
var xCanvasMax = 0;
var yCanvasMax = 0;
var xCanvasMid = 0;
var yCanvasMid = 0;

const testImg = document.getElementById("test_image");      //test image for initialization
testImg.style.visibility = 'hidden';
navigator.getUserMedia(
    { video: {} },
    stream => webcam.srcObject = stream,
    err => console.error('Error in loading webcam: ', err)
)

Promise.all([
    console.log('Loading face api models...'),
    faceapi.nets.faceExpressionNet.loadFromUri('face_integration/tensorflow/face_api/models'),
    faceapi.nets.ssdMobilenetv1.loadFromUri('face_integration/tensorflow/face_api/models'),
    faceapi.nets.faceLandmark68TinyNet.loadFromUri('face_integration/tensorflow/face_api/models'),
    faceapi.nets.faceLandmark68Net.loadFromUri('face_integration/tensorflow/face_api/models'),
])
    .then(() => console.log("Face API models loaded."))
    .then(async () => {
        let face = await faceapi.detectSingleFace(testImg, new faceapi.SsdMobilenetv1Options());
        console.log("Test initialization: ", JSON.stringify(face));
    })
    .then(() => {
        //defining a sleep function here
        webcam.addEventListener('play', async () => {
            const sleep = ms => new Promise(res => setTimeout(res, ms));
            let dontStop = true;
            while (dontStop) {
                try {
                    await face_analyze();
                    await sleep(30);
                }
                catch (e) {
                    console.warn("Error in face analyse function: ", e.message);
                }
            }
        })
    })

//we provide the webcam element which is to be stopped
function stopwebcam(webcam) {
    var stream = webcam.srcObject;
    var tracks = stream.getTracks();
    for (var i = 0; i < tracks.length; i++) {
        var track = tracks[i];
        track.stop();
    }
    webcam.srcObject = null;
}

async function face_analyze(drawDetections = true, useTinyModel = true) {
    let facedetections = await faceapi.detectSingleFace(webcam)
        .withFaceLandmarks(useTinyModel)
        .withFaceExpressions();
    if (drawDetections) {
        const displaySize = { width: webcam.width, height: webcam.height }
        const canvas = document.getElementById('image_overlay');
        faceapi.matchDimensions(canvas, displaySize)
        const resizedResults = faceapi.resizeResults(facedetections, displaySize)
        faceapi.draw.drawDetections(canvas, resizedResults)
        const res = moveFacePos(facedetections);
        /*
        document.getElementById("results").innerHTML =
            `X: ${res._face._x} Y: ${res._face._y} 
                <br> Mouth: ${res._mouth._extent}
                <br> Emotions: 
                <br> Sadness: ${facedetections.expressions.sad}
                <br> Joy: ${facedetections.expressions.happy}
                <br> Fear: ${facedetections.expressions.fearful}
                <br> Disgust: ${facedetections.expressions.disgusted}
                <br> Anger: ${facedetections.expressions.angry}
                <br> Surprise: ${facedetections.expressions.surprised}
                <br> Neutral: ${facedetections.expressions.neutral}
                `*/
        //now we change the mouse movements
        //xCanvasValue = scale(res._face._x * multFactor, 0, 100 * multFactor, 0, window.screen.availWidth);
        //yCanvasValue = scale(res._face._y * multFactor, 0, 100 * multFactor, window.screen.availHeight, 0);
        if (res._face._x !== undefined && !isNaN(res._face._x)) xCanvasValue = res._face._x;
        else {
            xCanvasValue = (xCanvasMin + xCanvasMax) / 2;
            console.warn("x coordinate invalid. Autogenerated x: ", xCanvasValue)
        }
        if (res._face._y !== undefined && !isNaN(res._face._y)) yCanvasValue = res._face._y;
        else {
            yCanvasValue = (yCanvasMax + yCanvasMin) / 3;
            console.warn("y coordinate invalid. Autogenerated y: ", yCanvasValue);
        }
        if (res._mouth._extent !== undefined && !isNaN(res._mouth._extent)) rmsValue = res._mouth._extent;          //we declared the rms value in model.ts
        else console.error("Mouth extent is invalid.")

        currentExpression = getExpression(facedetections);
        //we declared the current expression in model
    }
}

//updateSlider();

async function updateSlider() {
    let rmsElement = document.getElementById("rms");
    let emotionElement = document.getElementById("emotion");
    let xCoordElement = document.getElementById("xCoord");
    let yCoordElement = document.getElementById("yCoord");
    const sleep = ms => new Promise(req => setTimeout(req, ms));
    let everythingOk = true
    while (everythingOk) {
        try {
            xCoordElement.min = xCanvasMin;
            xCoordElement.max = xCanvasMax;
            xCoordElement.value = xCanvasValue;
            yCoordElement.min = yCanvasMin;
            yCoordElement.min = yCanvasMax;
            yCoordElement.value = yCanvasValue;
            console.log("yCanvasValue: ", yCanvasValue, " yMin: ", yCanvasMin, " yMax: ", yCanvasMax);
            rmsElement.value = rmsValue;
            //update the sliders every 20 seconds
            await sleep(20);
        }
        catch (e) {
            console.error("Error in update slider: ", e.message)
            everythingOk = false;
        }
    }
}

//now we can finally show the emotions on the face too
function getExpression(faceapi_data) {
    let emotion_array = new Array(8);
    emotion_array.fill(0);                          //initially fill the values with 0
    emotion_array[4] = faceapi_data.expressions.sad;
    emotion_array[5] = faceapi_data.expressions.happy;
    emotion_array[2] = faceapi_data.expressions.fearful;
    emotion_array[8] = faceapi_data.expressions.disgusted;
    emotion_array[4] = faceapi_data.expressions.angry;
    emotion_array[3] = faceapi_data.expressions.surprised;
    emotion_array[6] = faceapi_data.expressions.neutral;
    let expressionIndex = emotion_array.indexOf(Math.max(...emotion_array));
    //further modifications for the happy state
    if (expressionIndex == 5) {
        if (emotion_array[5] < 0.7) {
            expressionIndex = 1;
        }
        if (Math.abs(emotion_array[5] - emotion_array[3]) < 0.1) {
            expressionIndex = 7;
        }
    }
    return "F0" + expressionIndex;
}

function scale(x, in_min, in_max, out_min, out_max) {
    return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min;
}

const diffArrayLength = 5;
const diffFactor = 3.14;             //higher means smoother lower means faster
const absArrayLength = 5;
const absFactor = 2;

let prevMouthLength = 0;
//these ones store the difference values
let prevXDiff = new Array(diffArrayLength);
let prevY = new Array(diffArrayLength);
let diffWeights = [];
let absWeights = [];

function initFacePos() {
    const xInitValue = (xCanvasMax + xCanvasMin) / 2;
    const yInitValue = (yCanvasMax + yCanvasMin) / 2;
    prevXDiff.fill(xInitValue);
    prevY.fill(yInitValue);
    let remaining = 1;
    for (var i = 0; i < diffArrayLength; i++) {
        let current_weight = remaining / diffFactor;
        remaining = remaining - current_weight;
        diffWeights[i] = current_weight;
    }

    remaining = 1;
    for (var i = 0; i < absArrayLength; i++) {
        let current_weight = remaining / absFactor;
        remaining = remaining - current_weight;
        absWeights[i] = current_weight;
    }

    diffWeights.reverse();                 //also reversing the diff array    
    absWeights.reverse();
}

initFacePos();
function moveFacePos(facedata = {}) {
    const leftright_error = 100;
    const prevWeight = 0.4;
    let rightMargin = facedata.detection._box._x;
    let leftMargin = rightMargin - facedata.detection._box._width;
    const nosePt = facedata.landmarks._positions[30];
    const leftPt = facedata.landmarks._positions[2];
    const rightPt = facedata.landmarks._positions[14];
    const topPt = {
        _x: (facedata.landmarks._positions[21]._x + facedata.landmarks._positions[22]._x) / 2,
        _y: (facedata.landmarks._positions[21]._y + facedata.landmarks._positions[22]._y) / 2
    }
    const bottomPt = facedata.landmarks._positions[8]
    let topMargin = facedata.detection._box._y;
    //let topMargin = topPt._y        //maybe this is the best experimentally
    let bottomMargin = facedata.detection._box._y + facedata.detection._box._height;
    let leftLength = nosePt._x - leftPt._x;
    let rightLength = rightPt._x - nosePt._x;
    const maxXLength = rightMargin - leftMargin;
    const maxYLength = (bottomMargin - topMargin) * 0.3;
    //let topLength = nosePt._y - topPt._y;
    let topLength = topPt._y - topMargin;
    topLength = Math.min(maxYLength, topLength)
    //let bottomLength = bottomPt._y - nosePt._y;         //experimentally, the top length is a more accurate measure for the head turn
    //make sure we don't exceed the boundaries in case the head is tilted
    leftLength = Math.min(leftLength, maxXLength);
    rightLength = Math.min(rightLength, maxXLength)
    //bottomLength = Math.min(bottomLength, maxYLength);
    let leftXCoord = scale(leftLength, 0, maxXLength, xCanvasMin, xCanvasMax);
    let rightXCoord = scale(rightLength, 0, maxXLength, xCanvasMax, xCanvasMin);;
    let xCoord = 0;
    if (Math.abs(leftXCoord - rightXCoord) >= leftright_error) {
        //use the smaller length for more precision
        if (leftXCoord < rightXCoord) xCoord = leftXCoord;
        else xCoord = rightXCoord;
    }
    else xCoord = (leftXCoord + rightXCoord) / 2;

    let xDiff = xCoord - xCanvasValue;
    let currentXDiff = xDiff;
    prevXDiff.shift();
    prevXDiff.push(xDiff);
    xDiff = 0;

    diffWeights.forEach((el, i) => {
        //for the x coordinate
        if (Math.abs(Math.abs(currentXDiff) - Math.abs(prevXDiff[i])) < 0.1 * Math.abs(currentXDiff)) {
            //if the values are increasing or decreasing continuously, then we have nothing to do here
            let const_sign = true;
            for (let i = prevXDiff.length - 1; i > prevXDiff.length - 4; i--) {
                if (prevXDiff[prevXDiff.length - 1] > 0) {
                    if (prevXDiff[i] < 0) {
                        const_sign = false;
                        break;
                    }
                }
                else if (prevXDiff[prevXDiff.length - 1] < 0) {
                    if (prevXDiff[i] > 0) {
                        const_sign = false;
                        break;
                    }
                }
            }
            //only remove the oscillations not the slight increase in values
            if (const_sign) xDiff += el * Math.abs(prevXDiff[i]);
            else xDiff += 0.05 * Math.abs(prevXDiff[i]);
        }
        else xDiff += el * Math.abs(prevXDiff[i]);
    })
    let finalX = (xCoord > xCanvasValue) ? xCanvasValue + xDiff : xCanvasValue - xDiff;
    finalX = Math.min(xCanvasMax, finalX);
    finalX = Math.max(xCanvasMin, finalX);

    console.log("toplength: ", topLength)
    topLength = Math.max(topLength, maxYLength * 0.8);
    topLength = Math.min(topLength, maxYLength)
    //min 20
    console.log("MaX Y length: ", maxYLength)
    console.log("Min Y length: ", maxYLength * 0.8)
    let finalY = scale(topLength, maxYLength * 0.8, maxYLength, yCanvasMin, yCanvasMax)

    //mouth movements
    let mouthExtent = 0;
    const minMouth = 0;                 //experimentally determined values
    const maxMouth = 1.3;
    const noseTip = facedata.landmarks._positions[32];
    const chinTip = facedata.landmarks._positions[7];
    const upperMouthTip = facedata.landmarks._positions[61]; //or 50
    const lowerMouthTip = facedata.landmarks._positions[65]; //or 56
    const maxMouthLength = chinTip._y - noseTip._y;
    let mouthLength = lowerMouthTip._y - upperMouthTip._y;
    mouthExtent = scale(mouthLength, 0, maxMouthLength, minMouth - 1, maxMouth + 5);
    mouthExtent = Math.min(mouthExtent, maxMouth)
    mouthExtent = Math.max(mouthExtent, minMouth)
    //mouthExtent = prevMouthLength * prevWeight + mouthExtent;       //streamline the data
    return {
        _face: {
            _x: finalX,
            _y: finalY
        },
        _mouth: {
            _extent: mouthExtent
        }
    }
}

document.getElementById("speechButton").addEventListener("click", async () => {
    const sleep = ms => new Promise(req => setTimeout(req, ms));
    //Hi there! I am Mou. I would love to hang out with you. Would you consider me as your friend,
    //Hi! You are Neeharika Raa Bha. right, My creator Devashish has told me a lot about you, I would love to meet you sometime in real life!
    //const speakText = 'Hi there! Am I speaking to Neeharika Raa Bha, I am MOU, the AI created by Devashish. He has told me a lot about you, like you do amazing Bihu dance, I would love to meet you sometime in real life!'
    const speakText = "Hi there! I am Mou. I would love to hang out with you. Would you consider me as your friend,"
    const speech_results = document.getElementById("speech_output");
    speakSynth(speakText, setVoice(1));
    //MOU: Hi there! I am Mou. I would love to hang out <br> with you. Would you consider me as your friend,
    const displayTextArray = [
        "Hi there! I am Mou.",
        "I would love to hang out with you.",
        "Would you consider me as your friend,"
    ]
    /*
    const displayTextArray = [
        'Hi there!',
        'Am I speaking to Neeharika Rabha?',
        'I am Mou, the AI created by Debashish',
        "He has told me a lot about you.",
        "like you do amazing Bihu dance",
        "I would love to meet you sometime in real life!",
    ]*/

    const singleWordTime = 0.57;    //delay in seconds for every word
    let lastLipSyncTime = Date.now();
    const lipSyncDelay = 350;
    if (ifSpeaking()) {
        (async () => {
            for (let i = 0; i < displayTextArray.length; i++) {
                let numWords = displayTextArray[i].split(" ").length;
                speech_results.innerHTML = "MOU: " + displayTextArray[i];
                await sleep(singleWordTime * numWords * 1000)
                //remove the text after we have spoken it
                speech_results.innerHTML = "";
            }
        })();
    }
    while (ifSpeaking()) {
        //update the rms value when speakin
        let currentLipSyncTime = Date.now();
        if (currentLipSyncTime - lastLipSyncTime >= lipSyncDelay) {
            rmsValue = getRandomFloat(0.3, 0.5);
            lastLipSyncTime = currentLipSyncTime;
        }
        await sleep(10);
    }
    speech_results.innerText = "";
    function getRandomFloat(min, max, decimals = 2) {
        const str = (Math.random() * (max - min) + min).toFixed(decimals);
        return parseFloat(str);
    }
})